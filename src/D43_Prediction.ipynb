{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor, HistGradientBoostingRegressor,\n",
    "    RandomForestRegressor, ExtraTreesRegressor,\n",
    "    StackingRegressor, VotingRegressor, BaggingRegressor,\n",
    "    IsolationForest\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    Ridge, HuberRegressor, QuantileRegressor, \n",
    "    BayesianRidge, ElasticNet\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, cross_val_predict, KFold,\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, mean_absolute_error,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    RobustScaler, StandardScaler, QuantileTransformer,\n",
    "    PowerTransformer\n",
    ")\n",
    "from sklearn.feature_selection import mutual_info_regression, RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import iqr, norm\n",
    "from scipy.optimize import minimize_scalar\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58547e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = #HIDDEN\n",
    "combined_data = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUALITES = #HIDDEN\n",
    "OUTPUT_DIR = #HIDDEN\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FEATURES_POOL = [\n",
    "    #HIDDEN\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = {\n",
    "    #HIDDEN\n",
    "}\n",
    "\n",
    "ALL_CATEGORICAL = #HIDDEN\n",
    "\n",
    "SEUILS_PRODUCTION = {\n",
    "    'rmse_max': #HIDDEN,\n",
    "    'mae_max': #HIDDEN,\n",
    "    'erreur_max_acceptable': #HIDDEN,\n",
    "    'pct_within_5_min': #HIDDEN,\n",
    "    'confidence_threshold': #HIDDEN,\n",
    "}\n",
    "\n",
    "CIBLES = {\n",
    "    #HIDDEN\n",
    "}\n",
    "\n",
    "TOLERANCE_TERRAIN = #HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(y_true, y_pred):\n",
    "    return -np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def max_error_score(y_true, y_pred):\n",
    "    return -np.max(np.abs(y_true - y_pred))\n",
    "\n",
    "def p95_error_score(y_true, y_pred):\n",
    "    return -np.percentile(np.abs(y_true - y_pred), 95)\n",
    "\n",
    "def robust_score(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    p95 = np.percentile(np.abs(y_true - y_pred), 95)\n",
    "    max_err = np.max(np.abs(y_true - y_pred))\n",
    "    return -(0.5 * rmse + 0.3 * p95 + 0.2 * max_err)\n",
    "\n",
    "rmse_scorer = make_scorer(rmse_score, greater_is_better=True)\n",
    "robust_scorer = make_scorer(robust_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b611c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder:\n",
    "    \"\"\"Encodeur robuste pour variables catÃ©gorielles avec Target Encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing: float = 10.0):\n",
    "        self.smoothing = smoothing\n",
    "        self.encodings = {}\n",
    "        self.global_means = {}\n",
    "        self.category_counts = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, categorical_cols: List[str]):\n",
    "        self.categorical_cols = [c for c in categorical_cols if c in X.columns]\n",
    "        \n",
    "        if not self.categorical_cols:\n",
    "            self.is_fitted = True\n",
    "            return self\n",
    "        \n",
    "        global_mean = y.mean()\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            self.global_means[col] = global_mean\n",
    "            self.encodings[col] = {}\n",
    "            self.category_counts[col] = {}\n",
    "            \n",
    "            df_temp = pd.DataFrame({'cat': X[col], 'target': y})\n",
    "            stats = df_temp.groupby('cat')['target'].agg(['mean', 'count'])\n",
    "            \n",
    "            for category, row in stats.iterrows():\n",
    "                cat_mean = row['mean']\n",
    "                cat_count = row['count']\n",
    "                smoothed_mean = (\n",
    "                    (cat_count * cat_mean + self.smoothing * global_mean) / \n",
    "                    (cat_count + self.smoothing)\n",
    "                )\n",
    "                self.encodings[col][category] = smoothed_mean\n",
    "                self.category_counts[col][category] = cat_count\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = X.copy()\n",
    "        \n",
    "        if not self.is_fitted or not self.categorical_cols:\n",
    "            return X_out\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            if col not in X_out.columns:\n",
    "                continue\n",
    "            \n",
    "            encoded_col = f'{col}_encoded'\n",
    "            X_out[encoded_col] = X_out[col].map(self.encodings[col])\n",
    "            X_out[encoded_col] = X_out[encoded_col].fillna(self.global_means[col])\n",
    "            X_out[f'{col}_unknown'] = (~X_out[col].isin(self.encodings[col])).astype(int)\n",
    "            \n",
    "            total_count = sum(self.category_counts[col].values())\n",
    "            rare_threshold = total_count * 0.05\n",
    "            rare_categories = [\n",
    "                cat for cat, count in self.category_counts[col].items() \n",
    "                if count < rare_threshold\n",
    "            ]\n",
    "            X_out[f'{col}_rare'] = X_out[col].isin(rare_categories).astype(int)\n",
    "            X_out = X_out.drop(columns=[col])\n",
    "        \n",
    "        return X_out\n",
    "    \n",
    "    def fit_transform(self, X: pd.DataFrame, y: pd.Series, categorical_cols: List[str]) -> pd.DataFrame:\n",
    "        return self.fit(X, y, categorical_cols).transform(X)\n",
    "    \n",
    "    def get_category_stats(self, col: str) -> pd.DataFrame:\n",
    "        if col not in self.encodings:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'category': list(self.encodings[col].keys()),\n",
    "            'encoded_value': list(self.encodings[col].values()),\n",
    "            'count': [self.category_counts[col].get(k, 0) for k in self.encodings[col].keys()]\n",
    "        }).sort_values('encoded_value', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51690d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Handles feature engineering with a focus on robustness against outliers.\n",
    "    Computes statistics based on Median/IQR rather than Mean/Std.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_stats = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
    "        \"\"\"\n",
    "        Compute robust statistics (Median, IQR, Quartiles) for numeric columns.\n",
    "        These are stored to ensure consistent scaling during inference.\n",
    "        \"\"\"\n",
    "        for col in X.columns:\n",
    "            # Skip categorical/object types to prevent type errors\n",
    "            if X[col].dtype in ['object', 'category']:\n",
    "                continue\n",
    "                \n",
    "            # Storing stats for Robust Scaler logic later\n",
    "            self.feature_stats[col] = {\n",
    "                'median': X[col].median(),\n",
    "                'iqr': iqr(X[col].dropna()),  # Dropna strictly required for scipy iqr\n",
    "                'q1': X[col].quantile(0.25),\n",
    "                'q3': X[col].quantile(0.75),\n",
    "                'min': X[col].min(),\n",
    "                'max': X[col].max()\n",
    "            }\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = X.copy()\n",
    "        \n",
    "        # Interaction features\n",
    "        interactions = [\n",
    "                #HIDDEN\n",
    "        ]\n",
    "        \n",
    "        for f1, f2 in interactions:\n",
    "            if f1 in X_out.columns and f2 in X_out.columns:\n",
    "                # Ensure we only multiply numeric features\n",
    "                if X_out[f1].dtype not in ['object', 'category'] and X_out[f2].dtype not in ['object', 'category']:\n",
    "                    X_out[f'{f1}_x_{f2}'] = X_out[f1] * X_out[f2]\n",
    "        \n",
    "        # Gap analysis (ratios and diffs)\n",
    "        gap_pairs = #HIDDEN\n",
    "        \n",
    "        for gd, gg in gap_pairs:\n",
    "            if gd in X_out.columns and gg in X_out.columns:\n",
    "                prefix = gd[:5]\n",
    "                # Basic differential\n",
    "                X_out[f'{prefix}_diff'] = X_out[gd] - X_out[gg]\n",
    "                \n",
    "                # Ratio with epsilon (1e-6) to prevent DivisionByZero errors\n",
    "                X_out[f'{prefix}_ratio'] = X_out[gd] / (X_out[gg] + 1e-6)\n",
    "                \n",
    "                # Aggregations to capture range and center\n",
    "                X_out[f'{prefix}_mean'] = (X_out[gd] + X_out[gg]) / 2\n",
    "                X_out[f'{prefix}_max'] = np.maximum(X_out[gd], X_out[gg])\n",
    "                X_out[f'{prefix}_min'] = np.minimum(X_out[gd], X_out[gg])\n",
    "        \n",
    "        # Trend analysis\n",
    "        # Checks if all required hidden columns exist before computing trends\n",
    "        if all(f'#HIDDEN' in X_out.columns for i in [1, 2, 3]):\n",
    "            # Calculate instant delta\n",
    "            X_out['#HIDDEN'] = X_out['#HIDDEN'] - X_out['#HIDDEN']\n",
    "            # Calculate a simple moving average (window=3)\n",
    "            X_out['#HIDDEN'] = (\n",
    "                X_out['#HIDDEN'] + X_out['#HIDDEN'] + X_out['#HIDDEN']\n",
    "            ) / 3\n",
    "        \n",
    "        #  Physics-based deatures\n",
    "        if '#HIDDEN' in X_out.columns:\n",
    "            # Safe division for rate calculation\n",
    "            if '#HIDDEN' in X_out.columns:\n",
    "                X_out['#HIDDEN'] = X_out['#HIDDEN'] / (X_out['#HIDDEN'] + 1e-6)\n",
    "            \n",
    "            # Non-linear transformation (square)\n",
    "            X_out['#HIDDEN'] = X_out['#HIDDEN'] ** 2\n",
    "            \n",
    "            # Interaction term if specific column exists\n",
    "            if '#HIDDEN' in X_out.columns:\n",
    "                X_out['#HIDDEN'] = X_out['#HIDDEN'] * X_out['#HIDDEN']\n",
    "        \n",
    "        # Outlier detection\n",
    "        if self.is_fitted:\n",
    "            for col in ['#HIDDEN', '#HIDDEN', '#HIDDEN', '#HIDDEN']:\n",
    "                if col in X_out.columns and col in self.feature_stats:\n",
    "                    stats = self.feature_stats[col]\n",
    "                    \n",
    "                    # Apply robust Z-Score: (x - median) / IQR\n",
    "                    if stats['iqr'] > 0:\n",
    "                        X_out[f'{col}_zscore_robust'] = (\n",
    "                            (X_out[col] - stats['median']) / stats['iqr']\n",
    "                        )\n",
    "                    \n",
    "                    # Flag outliers\n",
    "                    X_out[f'{col}_outlier_flag'] = (\n",
    "                        (X_out[col] < stats['q1'] - 1.5 * stats['iqr']) |\n",
    "                        (X_out[col] > stats['q3'] + 1.5 * stats['iqr'])\n",
    "                    ).astype(int)\n",
    "        \n",
    "        # Polynomial features\n",
    "        for col in ['#HIDDEN', '#HIDDEN']:\n",
    "            if col in X_out.columns:\n",
    "                X_out[f'{col}_sq'] = X_out[col] ** 2\n",
    "        \n",
    "        return X_out\n",
    "    \n",
    "    def fit_transform(self, X: pd.DataFrame, y: pd.Series = None) -> pd.DataFrame:\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed59d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrictOutOfDomainDetector:\n",
    "    \"\"\"\n",
    "    Detects if new observations are outside the training domain.\n",
    "    Combines strict quantile bounding with isolation forest for robust rejection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        contamination: float = 0.05,\n",
    "        quantile_low: float = 0.05,\n",
    "        quantile_high: float = 0.95,\n",
    "        strict_mode: bool = False,\n",
    "        n_features_tolerance: int = 0\n",
    "    ):\n",
    "        self.contamination = contamination\n",
    "        self.quantile_low = quantile_low\n",
    "        self.quantile_high = quantile_high\n",
    "        self.strict_mode = strict_mode\n",
    "        self.n_features_tolerance = n_features_tolerance\n",
    "        \n",
    "        # isolation forest handles multivariate anomalies\n",
    "        self.isolation_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        # robust scaler minimizes the effect of outliers on distance calculations\n",
    "        self.scaler = RobustScaler()\n",
    "        self.train_centroid = None\n",
    "        self.train_std = None\n",
    "        self.feature_bounds = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\"Learns the statistical boundaries and fits the anomaly detector.\"\"\"\n",
    "        self.feature_names = list(X.columns)\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.isolation_forest.fit(X_scaled)\n",
    "        \n",
    "        # compute centroid on scaled space for distance-based rejection later\n",
    "        self.train_centroid = np.median(X_scaled, axis=0)\n",
    "        self.train_std = np.std(X_scaled, axis=0)\n",
    "        \n",
    "        # calculate hard acceptance boundaries per feature (raw scale)\n",
    "        print(f\"\\n   ðŸ“ feature bounds (Q{self.quantile_low*100:.0f}% - Q{self.quantile_high*100:.0f}%):\")\n",
    "        for col in X.columns:\n",
    "            q_low = X[col].quantile(self.quantile_low)\n",
    "            q_high = X[col].quantile(self.quantile_high)\n",
    "            self.feature_bounds[col] = {\n",
    "                'min': q_low,\n",
    "                'max': q_high,\n",
    "                'median': X[col].median(),\n",
    "                'std': X[col].std()\n",
    "            }\n",
    "            print(f\"      â€¢ {col}: [{q_low:.4f}, {q_high:.4f}]\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def check_bounds(self, X: pd.DataFrame) -> Tuple[np.ndarray, List[List[str]]]:\n",
    "        \"\"\"Checks if features are strictly within the learned quantiles.\"\"\"\n",
    "        n_samples = len(X)\n",
    "        n_violations = np.zeros(n_samples)\n",
    "        features_out_of_bounds = [[] for _ in range(n_samples)]\n",
    "        \n",
    "        for col in X.columns:\n",
    "            if col in self.feature_bounds:\n",
    "                bounds = self.feature_bounds[col]\n",
    "                \n",
    "                # vectorised check for efficiency\n",
    "                too_low = X[col] < bounds['min']\n",
    "                too_high = X[col] > bounds['max']\n",
    "                violations = too_low | too_high\n",
    "                \n",
    "                n_violations += violations.astype(int)\n",
    "                \n",
    "                # collect verbose error messages for audit logs\n",
    "                for i in range(n_samples):\n",
    "                    # robust indexing check (handles both series and numpy arrays)\n",
    "                    if violations.iloc[i] if hasattr(violations, 'iloc') else violations[i]:\n",
    "                        val = X[col].iloc[i] if hasattr(X[col], 'iloc') else X[col][i]\n",
    "                        if too_low.iloc[i] if hasattr(too_low, 'iloc') else too_low[i]:\n",
    "                            features_out_of_bounds[i].append(f\"{col}={val:.4f} < {bounds['min']:.4f}\")\n",
    "                        else:\n",
    "                            features_out_of_bounds[i].append(f\"{col}={val:.4f} > {bounds['max']:.4f}\")\n",
    "        \n",
    "        return n_violations, features_out_of_bounds\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[List[str]]]:\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            - confidence: composite score (0-1)\n",
    "            - should_reject_strict: boolean flag based on hard bounds\n",
    "            - features_out_of_bounds: details on violations\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # strict check on individual features first\n",
    "        n_violations, features_out_of_bounds = self.check_bounds(X)\n",
    "        should_reject_strict = n_violations > self.n_features_tolerance\n",
    "        \n",
    "        # isolation forest score (normalized to 0-1)\n",
    "        if_scores = self.isolation_forest.decision_function(X_scaled)\n",
    "        if_min, if_max = if_scores.min(), if_scores.max()\n",
    "        if if_max - if_min > 1e-10:\n",
    "            if_confidence = (if_scores - if_min) / (if_max - if_min)\n",
    "        else:\n",
    "            if_confidence = np.ones(len(X))\n",
    "        \n",
    "        # geometric distance from training centroid\n",
    "        distances = np.sqrt(np.sum((X_scaled - self.train_centroid) ** 2, axis=1))\n",
    "        max_dist = np.percentile(distances, 99) # clip extreme outliers\n",
    "        dist_confidence = 1 - np.clip(distances / max_dist, 0, 1)\n",
    "        \n",
    "        # bound violation penalty\n",
    "        max_violations = len(self.feature_bounds)\n",
    "        bound_confidence = 1 - np.clip(n_violations / max(max_violations, 1), 0, 1)\n",
    "        \n",
    "        # weighted ensemble of the three metrics\n",
    "        # heavy weight on hard bounds (0.4) vs statistical outliers (0.3 each)\n",
    "        confidence = 0.3 * if_confidence + 0.3 * dist_confidence + 0.4 * bound_confidence\n",
    "        \n",
    "        # force zero confidence if strict mode criteria are met\n",
    "        if self.strict_mode:\n",
    "            confidence[should_reject_strict] = 0.0\n",
    "        \n",
    "        return confidence, should_reject_strict, features_out_of_bounds\n",
    "    \n",
    "    def get_rejection_mask(self, X: pd.DataFrame, threshold: float = 0.5) -> Tuple[np.ndarray, List[List[str]]]:\n",
    "        \"\"\"Returns the final boolean decision mask.\"\"\"\n",
    "        confidence, should_reject_strict, features_out = self.predict(X)\n",
    "        \n",
    "        # if strict mode, reject on either strict flag or low confidence score\n",
    "        if self.strict_mode:\n",
    "            should_reject = should_reject_strict | (confidence < threshold)\n",
    "        else:\n",
    "            should_reject = confidence < threshold\n",
    "        \n",
    "        return should_reject, features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Ensemble wrapper that predicts the median and confidence intervals.\n",
    "    Uses HistGradientBoosting for efficient quantile regression on large datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators: int = 100, quantiles: List[float] = [0.1, 0.5, 0.9]):\n",
    "        self.n_estimators = n_estimators # note: currently unused, reserved for future bagging implementation\n",
    "        self.quantiles = quantiles\n",
    "        self.models = {}\n",
    "        self.base_model = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # training the primary median estimator with standard regularization\n",
    "        # used as the baseline for the main prediction\n",
    "        self.base_model = HistGradientBoostingRegressor(\n",
    "            loss='quantile',\n",
    "            quantile=0.5,\n",
    "            max_iter=200,\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=20,\n",
    "            learning_rate=0.05,\n",
    "            l2_regularization=1.0,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=15,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.base_model.fit(X, y)\n",
    "        \n",
    "        for q in self.quantiles:\n",
    "            # training specific quantile estimators (including bounds)\n",
    "            # using slightly higher regularization (l2=1.5) to ensure smoother bounds and reduce crossing\n",
    "            model = HistGradientBoostingRegressor(\n",
    "                loss='quantile',\n",
    "                quantile=q,\n",
    "                max_iter=150, # slightly fewer iterations to prevent overfitting extreme quantiles\n",
    "                max_depth=5,\n",
    "                min_samples_leaf=25,\n",
    "                learning_rate=0.05,\n",
    "                l2_regularization=1.5,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.1,\n",
    "                n_iter_no_change=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X, y)\n",
    "            self.models[q] = model\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        # defaults to the median prediction (q=0.5) for standard regression usage\n",
    "        return self.models[0.5].predict(X)\n",
    "    \n",
    "    def predict_quantiles(self, X) -> Dict[float, np.ndarray]:\n",
    "        # returns dictionary mapping quantile levels to their predictions\n",
    "        return {q: model.predict(X) for q, model in self.models.items()}\n",
    "    \n",
    "    def predict_with_uncertainty(self, X) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        # helper to unpack median, lower bound, and upper bound\n",
    "        preds = self.predict_quantiles(X)\n",
    "        median = preds[0.5]\n",
    "        # dynamically find widest interval requested (e.g. 10th and 90th percentiles)\n",
    "        lower = preds[min(self.quantiles)]\n",
    "        upper = preds[max(self.quantiles)]\n",
    "        return median, lower, upper\n",
    "    \n",
    "    def get_uncertainty(self, X) -> np.ndarray:\n",
    "        # returns the magnitude of the confidence interval (width)\n",
    "        # useful for filtering out low-confidence predictions\n",
    "        _, lower, upper = self.predict_with_uncertainty(X)\n",
    "        return upper - lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustStackingRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Stacking ensemble that combines diverse base models via a meta-learner.\n",
    "    Includes a post-training calibration step to correct systematic bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_models = {}\n",
    "        self.meta_model = None\n",
    "        self.scaler = RobustScaler()\n",
    "        self.calibration_params = {}\n",
    "    \n",
    "    def _create_base_models(self) -> Dict:\n",
    "        # mixing tree-based (non-linear) and linear models to maximize ensemble diversity\n",
    "        # huber helps with outliers, while boosting captures complex patterns\n",
    "        return {\n",
    "            'hgb_1': HistGradientBoostingRegressor(\n",
    "                max_iter=200, max_depth=5, min_samples_leaf=20,\n",
    "                learning_rate=0.05, l2_regularization=1.0,\n",
    "                early_stopping=True, validation_fraction=0.1,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'hgb_2': HistGradientBoostingRegressor(\n",
    "                max_iter=150, max_depth=7, min_samples_leaf=15,\n",
    "                learning_rate=0.03, l2_regularization=2.0,\n",
    "                early_stopping=True, validation_fraction=0.1,\n",
    "                random_state=43\n",
    "            ),\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=200, max_depth=8, min_samples_leaf=10,\n",
    "                max_features=0.5, max_samples=0.8,\n",
    "                random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'et': ExtraTreesRegressor(\n",
    "                n_estimators=200, max_depth=8, min_samples_leaf=10,\n",
    "                max_features=0.5, bootstrap=True,\n",
    "                random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'huber': Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('model', HuberRegressor(epsilon=1.35, alpha=0.001, max_iter=500))\n",
    "            ]),\n",
    "            'ridge': Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('model', Ridge(alpha=10.0))\n",
    "            ]),\n",
    "            'bayesian': Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('model', BayesianRidge())\n",
    "            ]),\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.base_models = self._create_base_models()\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        n_models = len(self.base_models)\n",
    "        # storage for out-of-fold predictions to train the meta-learner without leakage\n",
    "        oof_predictions = np.zeros((n_samples, n_models))\n",
    "        \n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        print(\"   EntraÃ®nement des modÃ¨les de base...\")\n",
    "        for i, (name, model) in enumerate(self.base_models.items()):\n",
    "            print(f\"      â€¢ {name}...\", end=\" \")\n",
    "            \n",
    "            oof_pred = np.zeros(n_samples)\n",
    "            \n",
    "            # generating oof predictions via cross-validation\n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                # safe indexing for both pandas dfs and numpy arrays\n",
    "                X_train_fold = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n",
    "                y_train_fold = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n",
    "                X_val_fold = X.iloc[val_idx] if hasattr(X, 'iloc') else X[val_idx]\n",
    "                \n",
    "                model_clone = clone(model)\n",
    "                model_clone.fit(X_train_fold, y_train_fold)\n",
    "                oof_pred[val_idx] = model_clone.predict(X_val_fold)\n",
    "            \n",
    "            oof_predictions[:, i] = oof_pred\n",
    "            rmse = np.sqrt(mean_squared_error(y, oof_pred))\n",
    "            print(f\"RMSE CV = {rmse:.3f}\")\n",
    "        \n",
    "        # retrain all base models on the full dataset for deployment\n",
    "        for name, model in self.base_models.items():\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        print(\"   EntraÃ®nement du meta-learner...\")\n",
    "        # ridge regression ensures stable coefficients even if base models are correlated\n",
    "        self.meta_model = Ridge(alpha=1.0)\n",
    "        self.meta_model.fit(oof_predictions, y)\n",
    "        \n",
    "        meta_predictions = self.meta_model.predict(oof_predictions)\n",
    "        self._calibrate(y, meta_predictions)\n",
    "        \n",
    "        print(f\"   Poids du meta-learner: {dict(zip(self.base_models.keys(), self.meta_model.coef_))}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _calibrate(self, y_true, y_pred):\n",
    "        # computes global bias to shift predictions if the ensemble systematically over/undershoots\n",
    "        residuals = y_true - y_pred\n",
    "        self.calibration_params = {\n",
    "            'bias': np.mean(residuals),\n",
    "            'scale': 1.0\n",
    "        }\n",
    "    \n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        # get predictions from all base models\n",
    "        base_preds = np.column_stack([\n",
    "            model.predict(X) for model in self.base_models.values()\n",
    "        ])\n",
    "        # combine using meta-learner weights\n",
    "        raw_pred = self.meta_model.predict(base_preds)\n",
    "        # apply final calibration shift\n",
    "        calibrated_pred = raw_pred + self.calibration_params['bias']\n",
    "        return calibrated_pred\n",
    "    \n",
    "    def predict_all_base(self, X) -> Dict[str, np.ndarray]:\n",
    "        # helper to inspect individual model contributions\n",
    "        return {name: model.predict(X) for name, model in self.base_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867db778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionD43Model(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Top-level wrapper for the production pipeline.\n",
    "    Enforces strict domain checks to prevent unreliable predictions on anomalous data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        rejection_threshold: float = 0.5, \n",
    "        qualite: str = None,\n",
    "        strict_mode: bool = True,\n",
    "        quantile_low: float = 0.05,\n",
    "        quantile_high: float = 0.95,\n",
    "        n_features_tolerance: int = 0\n",
    "    ):\n",
    "        self.rejection_threshold = rejection_threshold\n",
    "        self.qualite = qualite\n",
    "        self.strict_mode = strict_mode\n",
    "        self.quantile_low = quantile_low\n",
    "        self.quantile_high = quantile_high\n",
    "        self.n_features_tolerance = n_features_tolerance\n",
    "        \n",
    "        # initializing pipeline components\n",
    "        # smoothing=10 prevents overfitting on rare categories\n",
    "        self.categorical_encoder = CategoricalEncoder(smoothing=10.0)\n",
    "        self.feature_engineer = RobustFeatureEngineer()\n",
    "        \n",
    "        # the safety net: detects if input data is statistically similar to training data\n",
    "        self.ood_detector = StrictOutOfDomainDetector(\n",
    "            contamination=0.05,\n",
    "            quantile_low=quantile_low,\n",
    "            quantile_high=quantile_high,\n",
    "            strict_mode=strict_mode,\n",
    "            n_features_tolerance=n_features_tolerance\n",
    "        )\n",
    "        \n",
    "        # hybrid approach: stacking for accuracy, quantile ensemble for uncertainty estimation\n",
    "        self.stacking_model = RobustStackingRegressor()\n",
    "        self.quantile_model = QuantileEnsembleRegressor(quantiles=[0.1, 0.5, 0.9])\n",
    "        \n",
    "        self.feature_selector = None\n",
    "        self.selected_features = None\n",
    "        self.categorical_cols = []\n",
    "        self.training_stats = {}\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        print(f\"   â€¢ Mode strict: {self.strict_mode}\")\n",
    "        print(f\"   â€¢ Quantiles: [{self.quantile_low*100:.0f}%, {self.quantile_high*100:.0f}%]\")\n",
    "        print(f\"   â€¢ TolÃ©rance features hors bornes: {self.n_features_tolerance}\")\n",
    "        \n",
    "        # filter categorical columns based on the specific quality config\n",
    "        self.categorical_cols = CATEGORICAL_FEATURES.get(self.qualite, [])\n",
    "        self.categorical_cols = [c for c in self.categorical_cols if c in X.columns]\n",
    "        \n",
    "        if self.categorical_cols:\n",
    "            print(f\"\\n0ï¸âƒ£ Features catÃ©gorielles dÃ©tectÃ©es: {self.categorical_cols}\")\n",
    "        \n",
    "        print(\"\\n1ï¸âƒ£ Encodage des features catÃ©gorielles...\")\n",
    "        if self.categorical_cols:\n",
    "            X_encoded = self.categorical_encoder.fit_transform(X, y, self.categorical_cols)\n",
    "            # logging stats to monitor target encoding distribution\n",
    "            for col in self.categorical_cols:\n",
    "                stats = self.categorical_encoder.get_category_stats(col)\n",
    "                print(f\"\\n   ðŸ“Š Encodage {col}:\")\n",
    "                print(stats.to_string(index=False))\n",
    "        else:\n",
    "            X_encoded = X.copy()\n",
    "            print(\"   Aucune feature catÃ©gorielle\")\n",
    "        \n",
    "        print(\"\\n2ï¸âƒ£ Feature Engineering...\")\n",
    "        # generating domain-specific interactions and ratios\n",
    "        X_eng = self.feature_engineer.fit_transform(X_encoded, y)\n",
    "        print(f\"   {len(X_eng.columns)} features gÃ©nÃ©rÃ©es\")\n",
    "        \n",
    "        print(\"\\n3ï¸âƒ£ SÃ©lection de features...\")\n",
    "        # crucial step: reduce dimensionality to prevent the stacking ensemble from overfitting\n",
    "        self.selected_features = self._select_features(X_eng, y)\n",
    "        X_selected = X_eng[self.selected_features]\n",
    "        print(f\"   {len(self.selected_features)} features sÃ©lectionnÃ©es\")\n",
    "        \n",
    "        print(\"\\n4ï¸âƒ£ EntraÃ®nement du dÃ©tecteur hors-domaine (STRICT)...\")\n",
    "        # fitting the safety net on the final feature space\n",
    "        self.ood_detector.fit(X_selected)\n",
    "        \n",
    "        print(\"\\n5ï¸âƒ£ EntraÃ®nement de l'ensemble stacking...\")\n",
    "        self.stacking_model.fit(X_selected, y)\n",
    "        \n",
    "        print(\"\\n6ï¸âƒ£ EntraÃ®nement du modÃ¨le quantile...\")\n",
    "        self.quantile_model.fit(X_selected, y)\n",
    "        \n",
    "        # persist training metadata for reproducibility and audit trails\n",
    "        self.training_stats = {\n",
    "            'y_mean': float(y.mean()),\n",
    "            'y_std': float(y.std()),\n",
    "            'y_min': float(y.min()),\n",
    "            'y_max': float(y.max()),\n",
    "            'n_samples': len(y),\n",
    "            'categorical_cols': self.categorical_cols,\n",
    "            'n_features_total': len(X_eng.columns),\n",
    "            'n_features_selected': len(self.selected_features),\n",
    "            'strict_mode': self.strict_mode,\n",
    "            'quantile_low': self.quantile_low,\n",
    "            'quantile_high': self.quantile_high,\n",
    "            'n_features_tolerance': self.n_features_tolerance,\n",
    "            'feature_bounds': self.ood_detector.feature_bounds\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _select_features(self, X: pd.DataFrame, y: pd.Series, max_features: int = 15) -> List[str]:\n",
    "        # two-stage selection process: fast filter (mutual info) -> wrapper (forward selection)\n",
    "        \n",
    "        # 1. Mutual Information (fast) to filter out noise\n",
    "        mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "        mi_df = pd.DataFrame({'feature': X.columns, 'mi': mi_scores})\n",
    "        mi_df = mi_df.sort_values('mi', ascending=False)\n",
    "        \n",
    "        # keep top 70% features to reduce search space for the expensive wrapper method\n",
    "        threshold = mi_df['mi'].quantile(0.3)\n",
    "        preselected = mi_df[mi_df['mi'] > threshold]['feature'].tolist()\n",
    "        \n",
    "        # 2. Forward Selection (slow) optimizing for RMSE\n",
    "        selected = []\n",
    "        best_rmse = np.inf\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        available = preselected.copy()\n",
    "        \n",
    "        while available and len(selected) < max_features:\n",
    "            scores = {}\n",
    "            for feat in available:\n",
    "                candidate = selected + [feat]\n",
    "                \n",
    "                # using a lightweight model for speed during selection loop\n",
    "                model = HistGradientBoostingRegressor(\n",
    "                    max_iter=100, max_depth=5, min_samples_leaf=15,\n",
    "                    learning_rate=0.1, l2_regularization=1.0,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                cv_rmse = -cross_val_score(\n",
    "                    model, X[candidate], y, cv=kf,\n",
    "                    scoring='neg_root_mean_squared_error'\n",
    "                ).mean()\n",
    "                scores[feat] = cv_rmse\n",
    "            \n",
    "            # greedy selection of the best feature\n",
    "            best_feat = min(scores, key=scores.get)\n",
    "            \n",
    "            # early stopping if performance improvement is negligible (< 0.5%)\n",
    "            if scores[best_feat] < best_rmse * 0.995:\n",
    "                selected.append(best_feat)\n",
    "                available.remove(best_feat)\n",
    "                best_rmse = scores[best_feat]\n",
    "                print(f\"      + {best_feat:<30} RMSE={best_rmse:.4f}\")\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def _prepare_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # ensures inference pipeline matches training transformations exactly\n",
    "        if self.categorical_cols:\n",
    "            X_encoded = self.categorical_encoder.transform(X)\n",
    "        else:\n",
    "            X_encoded = X.copy()\n",
    "        \n",
    "        X_eng = self.feature_engineer.transform(X_encoded)\n",
    "        X_selected = X_eng[self.selected_features]\n",
    "        \n",
    "        return X_selected\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame, return_details: bool = False):\n",
    "        \"\"\"\n",
    "        Orchestrates prediction, uncertainty estimation, and domain rejection.\n",
    "        \"\"\"\n",
    "        X_selected = self._prepare_features(X)\n",
    "        \n",
    "        pred_stacking = self.stacking_model.predict(X_selected)\n",
    "        pred_median, pred_lower, pred_upper = self.quantile_model.predict_with_uncertainty(X_selected)\n",
    "        \n",
    "        # weighted ensemble: giving slightly more weight to the stacker (60%) for accuracy,\n",
    "        # but keeping median (40%) to stabilize predictions against outliers\n",
    "        predictions = 0.6 * pred_stacking + 0.4 * pred_median\n",
    "        uncertainty = pred_upper - pred_lower\n",
    "        \n",
    "        # check if data is out-of-domain (returns flags and strict bounds violations)\n",
    "        ood_confidence, should_reject_strict, features_out_of_bounds = self.ood_detector.predict(X_selected)\n",
    "        \n",
    "        # normalize uncertainty to a confidence score based on training std dev\n",
    "        uncertainty_confidence = 1 - np.clip(uncertainty / (self.training_stats['y_std'] * 2), 0, 1)\n",
    "        \n",
    "        # combined confidence score\n",
    "        confidence = 0.5 * ood_confidence + 0.5 * uncertainty_confidence\n",
    "        \n",
    "        # strict mode logic: if ANY feature is out of bounds, reject immediately\n",
    "        if self.strict_mode:\n",
    "            should_reject = should_reject_strict | (confidence < self.rejection_threshold)\n",
    "        else:\n",
    "            should_reject = confidence < self.rejection_threshold\n",
    "        \n",
    "        if return_details:\n",
    "            return {\n",
    "                'predictions': predictions,\n",
    "                'lower_bound': pred_lower,\n",
    "                'upper_bound': pred_upper,\n",
    "                'uncertainty': uncertainty,\n",
    "                'confidence': confidence,\n",
    "                'should_reject': should_reject,\n",
    "                'should_reject_strict': should_reject_strict,\n",
    "                'features_out_of_bounds': features_out_of_bounds,\n",
    "                'pred_stacking': pred_stacking,\n",
    "                'pred_quantile': pred_median\n",
    "            }\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_safe(self, X: pd.DataFrame, fallback_value: float = None) -> Tuple[np.ndarray, np.ndarray, List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Production-ready wrapper that handles rejections gracefully by substituting fallback values.\n",
    "        \"\"\"\n",
    "        details = self.predict(X, return_details=True)\n",
    "        \n",
    "        predictions = details['predictions'].copy()\n",
    "        is_reliable = ~details['should_reject']\n",
    "        features_out = details['features_out_of_bounds']\n",
    "        \n",
    "        if fallback_value is not None:\n",
    "            predictions[~is_reliable] = fallback_value\n",
    "        \n",
    "        return predictions, is_reliable, features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "def evaluer_modele_production(model, X_test, y_test, qualite: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Computes production metrics, distinguishing between all predictions \n",
    "    and 'reliable' predictions (those not rejected by the OOD detector).\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract full prediction details including uncertainty and rejection flags\n",
    "    details = model.predict(X_test, return_details=True)\n",
    "    predictions = details['predictions']\n",
    "    confidence = details['confidence']\n",
    "    should_reject = details['should_reject']\n",
    "    # fallback to general rejection if strict flag is missing\n",
    "    should_reject_strict = details.get('should_reject_strict', should_reject)\n",
    "    features_out = details.get('features_out_of_bounds', [])\n",
    "    \n",
    "    y_test_arr = y_test.values if hasattr(y_test, 'values') else np.array(y_test)\n",
    "    residuals = y_test_arr - predictions\n",
    "    abs_errors = np.abs(residuals)\n",
    "    \n",
    "    # standard regression metrics on the full dataset\n",
    "    metrics = {\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test_arr, predictions)),\n",
    "        'mae': mean_absolute_error(y_test_arr, predictions),\n",
    "        'r2': r2_score(y_test_arr, predictions),\n",
    "        'median_ae': np.median(abs_errors),\n",
    "        'p90_ae': np.percentile(abs_errors, 90),\n",
    "        'p95_ae': np.percentile(abs_errors, 95),\n",
    "        'p99_ae': np.percentile(abs_errors, 99),\n",
    "        'max_ae': np.max(abs_errors),\n",
    "        'biais': np.mean(residuals),\n",
    "    }\n",
    "    \n",
    "    # granular accuracy check: what % of predictions are within X units\n",
    "    for threshold in [1, 2, 3, 5, 10]:\n",
    "        metrics[f'pct_within_{threshold}'] = np.mean(abs_errors <= threshold) * 100\n",
    "    \n",
    "    n_total = len(y_test_arr)\n",
    "    n_rejected_strict = np.sum(should_reject_strict)\n",
    "    n_rejected_total = np.sum(should_reject)\n",
    "    \n",
    "    print(f\"RejetÃ©es (strict bounds): {n_rejected_strict} ({n_rejected_strict/n_total*100:.1f}%)\")\n",
    "    print(f\" RejetÃ©es (total): {n_rejected_total} ({n_rejected_total/n_total*100:.1f}%)\")\n",
    "    \n",
    "    # logging specific reasons for rejection (audit trail)\n",
    "    if n_rejected_strict > 0:\n",
    "        print(f\"\\n DÃ©tail des rejets stricts (features hors bornes):\")\n",
    "        rejected_indices = np.where(should_reject_strict)[0]\n",
    "        for idx in rejected_indices[:10]:\n",
    "            print(f\"      â€¢ Obs {idx}: {features_out[idx]}\")\n",
    "        if len(rejected_indices) > 10:\n",
    "            print(f\"      ... et {len(rejected_indices) - 10} autres\")\n",
    "    \n",
    "    # compute metrics specifically for the accepted (reliable) subset\n",
    "    # this shows the potential performance of the model in a live environment\n",
    "    if np.sum(~should_reject) > 0:\n",
    "        reliable_residuals = y_test_arr[~should_reject] - predictions[~should_reject]\n",
    "        reliable_abs_errors = np.abs(reliable_residuals)\n",
    "        \n",
    "        metrics['reliable_rmse'] = np.sqrt(np.mean(reliable_residuals ** 2))\n",
    "        metrics['reliable_mae'] = np.mean(reliable_abs_errors)\n",
    "        metrics['reliable_max_ae'] = np.max(reliable_abs_errors)\n",
    "        metrics['reliable_pct'] = np.mean(~should_reject) * 100\n",
    "        metrics['rejected_strict_pct'] = n_rejected_strict / n_total * 100\n",
    "        metrics['rejected_total_pct'] = n_rejected_total / n_total * 100\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    MÃ‰TRIQUES GLOBALES:\n",
    "    â€¢ RMSE          : {metrics['rmse']:.3f}\n",
    "    â€¢ MAE           : {metrics['mae']:.3f}\n",
    "    â€¢ RÂ²            : {metrics['r2']:.4f}\n",
    "    â€¢ Biais         : {metrics['biais']:.3f}\n",
    "    \n",
    "    DISTRIBUTION DES ERREURS:\n",
    "    â€¢ MÃ©diane       : {metrics['median_ae']:.2f}\n",
    "    â€¢ P90           : {metrics['p90_ae']:.2f}\n",
    "    â€¢ P95           : {metrics['p95_ae']:.2f}\n",
    "    â€¢ Max           : {metrics['max_ae']:.2f}\n",
    "    \n",
    "    POURCENTAGES:\n",
    "    â€¢ Erreur â‰¤ 1    : {metrics['pct_within_1']:.1f}%\n",
    "    â€¢ Erreur â‰¤ 2    : {metrics['pct_within_2']:.1f}%\n",
    "    â€¢ Erreur â‰¤ 3    : {metrics['pct_within_3']:.1f}%\n",
    "    â€¢ Erreur â‰¤ 5    : {metrics['pct_within_5']:.1f}%\n",
    "    â€¢ Erreur â‰¤ 10   : {metrics['pct_within_10']:.1f}%\n",
    "    \"\"\")\n",
    "    \n",
    "    if 'reliable_rmse' in metrics:\n",
    "        print(f\"\"\"\n",
    "    PRÃ‰DICTIONS FIABLES (non rejetÃ©es):\n",
    "    â€¢ Taux fiable   : {metrics['reliable_pct']:.1f}%\n",
    "    â€¢ Rejet strict  : {metrics['rejected_strict_pct']:.1f}% (features hors bornes)\n",
    "    â€¢ RMSE fiable   : {metrics['reliable_rmse']:.3f}\n",
    "    â€¢ MAE fiable    : {metrics['reliable_mae']:.3f}\n",
    "    â€¢ Max Err fiable: {metrics['reliable_max_ae']:.2f}\n",
    "        \"\"\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def diagnostic_production(y_true, details, qualite: str, save_dir: str = None, \n",
    "                          tolerance_modele: float = 5.0):\n",
    "    \"\"\"\n",
    "    Generates a 5-panel dashboard to visualize model reliability.\n",
    "    Focuses on classification of 'Flags' (alerts) using TP/FP/FN/TN logic.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = details['predictions']\n",
    "    confidence = details['confidence']\n",
    "    should_reject = details['should_reject']\n",
    "    should_reject_strict = details.get('should_reject_strict', should_reject)\n",
    "    uncertainty = details['uncertainty']\n",
    "    \n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "    y_pred_arr = predictions\n",
    "    residuals = y_true_arr - predictions\n",
    "    abs_errors = np.abs(residuals)\n",
    "    \n",
    "    # setting up target values and physical tolerances\n",
    "    cible = CIBLES_D43.get(qualite, 300)\n",
    "    tolerance_terrain = TOLERANCE_TERRAIN # physical limit on the field\n",
    "    tol_opt = tolerance_modele # optimized model threshold\n",
    "    \n",
    "    # split data into accepted (safe to predict) and rejected (OOD)\n",
    "    mask_accepted = ~should_reject\n",
    "    mask_rejected = should_reject\n",
    "    n_accepted = np.sum(mask_accepted)\n",
    "    n_rejected = np.sum(mask_rejected)\n",
    "    \n",
    "    y_true_accepted = y_true_arr[mask_accepted]\n",
    "    y_pred_accepted = y_pred_arr[mask_accepted]\n",
    "    y_true_rejected = y_true_arr[mask_rejected] if n_rejected > 0 else np.array([])\n",
    "    y_pred_rejected = y_pred_arr[mask_rejected] if n_rejected > 0 else np.array([])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # defining ground truth flags (what actually happened)\n",
    "    flags_reels_accepted = (y_true_accepted < cible - tolerance_terrain) | (y_true_accepted > cible + tolerance_terrain)\n",
    "    # defining predicted flags (what the model warned about)\n",
    "    flags_pred_accepted = (y_pred_accepted < cible - tol_opt) | (y_pred_accepted > cible + tol_opt)\n",
    "    \n",
    "    # logic masks for confusion matrix visualization\n",
    "    mask_TP = flags_reels_accepted & flags_pred_accepted\n",
    "    mask_TN = ~flags_reels_accepted & ~flags_pred_accepted\n",
    "    mask_FP = ~flags_reels_accepted & flags_pred_accepted\n",
    "    mask_FN = flags_reels_accepted & ~flags_pred_accepted\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "    fig.suptitle(f'Diagnostic Production (STRICT) - {qualite}\\n'\n",
    "                 f'Cible={cible}, Tol. terrain=Â±{tolerance_terrain}, Tol. modÃ¨le=Â±{tol_opt}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Panel 1: Main Scatter Plot\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    # rejected points in gray background\n",
    "    if n_rejected > 0:\n",
    "        ax1.scatter(y_true_rejected, y_pred_rejected, \n",
    "                   c='gray', marker='o', s=40, alpha=0.3, \n",
    "                   label=f'REJETÃ‰ES (n={n_rejected})', zorder=1)\n",
    "    \n",
    "    # plotting classification results (FN are large red X's as they are critical errors)\n",
    "    for mask, label, color, marker, size, zorder in [\n",
    "        (mask_TN, f'TN (n={np.sum(mask_TN)})', 'lightgreen', 'o', 40, 2),\n",
    "        (mask_TP, f'TP (n={np.sum(mask_TP)})', 'darkgreen', 'o', 60, 3),\n",
    "        (mask_FP, f'FP (n={np.sum(mask_FP)})', 'orange', 's', 80, 4),\n",
    "        (mask_FN, f'FN (n={np.sum(mask_FN)})', 'red', 'X', 150, 5),\n",
    "    ]:\n",
    "        if np.sum(mask) > 0:\n",
    "            ax1.scatter(y_true_accepted[mask], y_pred_accepted[mask], \n",
    "                       c=color, marker=marker, s=size, \n",
    "                       label=label, alpha=0.7, edgecolors='black', linewidths=0.5,\n",
    "                       zorder=zorder)\n",
    "    \n",
    "    # identity line and tolerance bands\n",
    "    all_y = np.concatenate([y_true_arr, y_pred_arr])\n",
    "    min_val, max_val = all_y.min() - 5, all_y.max() + 5\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    ax1.axvline(cible - tolerance_terrain, color='red', linestyle='--', linewidth=2, alpha=0.5, label=f'Bornes terrain Â±{tolerance_terrain}')\n",
    "    ax1.axvline(cible + tolerance_terrain, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "    ax1.axhline(cible - tol_opt, color='blue', linestyle=':', linewidth=2, alpha=0.5, label=f'Bornes modÃ¨le Â±{tol_opt}')\n",
    "    ax1.axhline(cible + tol_opt, color='blue', linestyle=':', linewidth=2, alpha=0.5)\n",
    "    \n",
    "    # visualising the \"safe zone\"\n",
    "    ax1.axhspan(cible - tol_opt, cible + tol_opt, alpha=0.1, color='blue')\n",
    "    ax1.axvspan(cible - tolerance_terrain, cible + tolerance_terrain, alpha=0.1, color='green')\n",
    "    \n",
    "    ax1.set_xlabel('Y RÃ©el (D43)', fontsize=12)\n",
    "    ax1.set_ylabel('Y PrÃ©dit', fontsize=12)\n",
    "    ax1.set_title(f'PrÃ©dictions avec Classification Flags\\n'\n",
    "                  f'(Gris=RejetÃ©es, Vert=OK, Rouge=FN manquÃ©s, Orange=FP fausses alertes)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.legend(loc='upper left', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    #Panel 2: Confusion Matrix\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    cm = np.array([[np.sum(mask_TN), np.sum(mask_FP)],\n",
    "                   [np.sum(mask_FN), np.sum(mask_TP)]])\n",
    "    \n",
    "    # hardcoded colors: Green for TP/TN, Orange/Red for FP/FN\n",
    "    cm_colors = np.array([['#90EE90', '#FFA500'],\n",
    "                          ['#FF4444', '#228B22']])\n",
    "    \n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax2.add_patch(plt.Rectangle((j, 1-i), 1, 1, fill=True, \n",
    "                                        color=cm_colors[i, j], alpha=0.7))\n",
    "            ax2.text(j + 0.5, 1.5 - i, f'{cm[i, j]}', \n",
    "                    ha='center', va='center', fontsize=24, fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlim(0, 2)\n",
    "    ax2.set_ylim(0, 2)\n",
    "    ax2.set_xticks([0.5, 1.5])\n",
    "    ax2.set_yticks([0.5, 1.5])\n",
    "    ax2.set_xticklabels(['PrÃ©dit OK', 'PrÃ©dit FLAG'], fontsize=11)\n",
    "    ax2.set_yticklabels(['RÃ©el FLAG', 'RÃ©el OK'], fontsize=11)\n",
    "    ax2.set_title(f'Matrice de Confusion (Flags)\\n'\n",
    "                  f'Sur {n_accepted} prÃ©dictions acceptÃ©es', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # manual calculation of precision/recall for annotation\n",
    "    precision = np.sum(mask_TP) / (np.sum(mask_TP) + np.sum(mask_FP)) if (np.sum(mask_TP) + np.sum(mask_FP)) > 0 else 0\n",
    "    recall = np.sum(mask_TP) / (np.sum(mask_TP) + np.sum(mask_FN)) if (np.sum(mask_TP) + np.sum(mask_FN)) > 0 else 0\n",
    "    ax2.text(1, -0.15, f'Precision: {precision:.1%} | Recall: {recall:.1%}', \n",
    "             ha='center', va='top', fontsize=10, transform=ax2.transAxes)\n",
    "    \n",
    "    #Panel 3: Error Distribution\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    accepted_mask = ~should_reject\n",
    "    rejected_bounds = should_reject_strict\n",
    "    \n",
    "    if np.sum(accepted_mask) > 0:\n",
    "        ax3.hist(abs_errors[accepted_mask], bins=30, alpha=0.7, \n",
    "                label=f'AcceptÃ©es (n={np.sum(accepted_mask)})', color='green', density=True)\n",
    "    if np.sum(rejected_bounds) > 0:\n",
    "        # showing that rejected items usually have higher errors\n",
    "        ax3.hist(abs_errors[rejected_bounds], bins=20, alpha=0.7, \n",
    "                label=f'Rejet bounds (n={np.sum(rejected_bounds)})', color='red', density=True)\n",
    "    \n",
    "    ax3.axvline(x=5, color='black', linestyle='--', linewidth=2, label='Seuil 5')\n",
    "    ax3.set_xlabel('Erreur Absolue', fontsize=11)\n",
    "    ax3.set_ylabel('DensitÃ©', fontsize=11)\n",
    "    ax3.set_title('Distribution des Erreurs\\npar Type de Rejection', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    if np.sum(accepted_mask) > 0:\n",
    "        mae_acc = abs_errors[accepted_mask].mean()\n",
    "        mae_rej = abs_errors[rejected_bounds].mean() if np.sum(rejected_bounds) > 0 else 0\n",
    "        ax3.text(0.95, 0.95, f'MAE acceptÃ©es: {mae_acc:.2f}\\nMAE rejetÃ©es: {mae_rej:.2f}',\n",
    "                transform=ax3.transAxes, ha='right', va='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    #Panel 4: Error vs Uncertainty (Calibration Check)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    colors_scatter = np.where(should_reject_strict, 'red', \n",
    "                              np.where(should_reject, 'orange', 'green'))\n",
    "    \n",
    "    # validates if higher uncertainty actually correlates with higher error\n",
    "    scatter = ax4.scatter(uncertainty, abs_errors, c=colors_scatter, alpha=0.6, \n",
    "                          edgecolors='black', linewidths=0.3, s=50)\n",
    "    ax4.axhline(y=5, color='red', linestyle='--', linewidth=2, label='Erreur = 5')\n",
    "    \n",
    "    # trend line to visualize the correlation\n",
    "    z = np.polyfit(uncertainty[~should_reject], abs_errors[~should_reject], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(uncertainty.min(), uncertainty.max(), 100)\n",
    "    ax4.plot(x_line, p(x_line), 'b--', alpha=0.5, label=f'Tendance (pente={z[0]:.2f})')\n",
    "    \n",
    "    ax4.set_xlabel('Incertitude (largeur intervalle)', fontsize=11)\n",
    "    ax4.set_ylabel('Erreur Absolue', fontsize=11)\n",
    "    ax4.set_title('Erreur vs Incertitude\\n(Vert=OK, Rouge=Hors bornes)', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Panel 5: Error vs Confidence Score\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    ax5.scatter(confidence[accepted_mask], abs_errors[accepted_mask], \n",
    "               c='green', alpha=0.5, s=50, label=f'AcceptÃ©es (n={np.sum(accepted_mask)})')\n",
    "    if np.sum(rejected_bounds) > 0:\n",
    "        ax5.scatter(confidence[rejected_bounds], abs_errors[rejected_bounds], \n",
    "                   c='red', alpha=0.7, marker='X', s=100, label=f'Hors bornes (n={np.sum(rejected_bounds)})')\n",
    "    \n",
    "    ax5.axhline(y=5, color='black', linestyle='--', alpha=0.5, label='Erreur = 5')\n",
    "    \n",
    "    # shading the low confidence zone\n",
    "    ax5.axvspan(0, 0.25, alpha=0.1, color='red', label='Zone low conf')\n",
    "    \n",
    "    ax5.set_xlabel('Confidence Score', fontsize=11)\n",
    "    ax5.set_ylabel('Erreur Absolue', fontsize=11)\n",
    "    ax5.set_title('Erreur vs Confidence\\n(Zone rouge = low confidence)', fontsize=12, fontweight='bold')\n",
    "    ax5.legend(fontsize=9, loc='upper right')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    corr = np.corrcoef(confidence[accepted_mask], abs_errors[accepted_mask])[0, 1]\n",
    "    ax5.text(0.05, 0.95, f'Corr: {corr:.3f}', transform=ax5.transAxes, \n",
    "             ha='left', va='top', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        filepath = os.path.join(save_dir, f'diagnostic_production_strict_{qualite}.png')\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def analyser_rejets_stricts(X, y_true, details, qualite, save_dir=None):\n",
    "    \"\"\"\n",
    "    Analyzes observations rejected due to out-of-bounds features.\n",
    "    Helps understand which features are driving the strict rejection.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "    predictions = details['predictions']\n",
    "    should_reject_strict = details.get('should_reject_strict', details['should_reject'])\n",
    "    features_out = details.get('features_out_of_bounds', [])\n",
    "    \n",
    "    n_total = len(y_true_arr)\n",
    "    n_rejected = np.sum(should_reject_strict)\n",
    "    \n",
    "    print(f\"Total: {n_total}\")\n",
    "    print(f\"RejetÃ©es (strict): {n_rejected} ({n_rejected/n_total*100:.1f}%)\")\n",
    "    \n",
    "    if n_rejected == 0:\n",
    "        print(\"\\n Aucune observation rejetÃ©e\")\n",
    "        return {'n_rejected': 0}\n",
    "    \n",
    "    # aggregating violation counts per feature\n",
    "    feature_violation_count = {}\n",
    "    for feats in features_out:\n",
    "        for feat_info in feats:\n",
    "            feat_name = feat_info.split('=')[0]\n",
    "            feature_violation_count[feat_name] = feature_violation_count.get(feat_name, 0) + 1\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Features hors bornes (top 10):\")\n",
    "    sorted_features = sorted(feature_violation_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    for feat, count in sorted_features[:10]:\n",
    "        pct = count / n_rejected * 100\n",
    "        print(f\"   â€¢ {feat}: {count} fois ({pct:.1f}%)\")\n",
    "    \n",
    "    # comparing error rates to justify the rejection\n",
    "    # usually rejected items have much higher RMSE, validating the strategy\n",
    "    abs_errors_rejected = np.abs(y_true_arr[should_reject_strict] - predictions[should_reject_strict])\n",
    "    abs_errors_accepted = np.abs(y_true_arr[~should_reject_strict] - predictions[~should_reject_strict])\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Comparaison erreurs:\")\n",
    "    print(f\"   â€¢ Erreur moyenne (rejetÃ©es): {abs_errors_rejected.mean():.2f}\")\n",
    "    print(f\"   â€¢ Erreur moyenne (acceptÃ©es): {abs_errors_accepted.mean():.2f}\")\n",
    "    print(f\"   â€¢ % erreur >= 5 (rejetÃ©es): {np.mean(abs_errors_rejected >= 5)*100:.1f}%\")\n",
    "    print(f\"   â€¢ % erreur >= 5 (acceptÃ©es): {np.mean(abs_errors_accepted >= 5)*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'n_rejected': n_rejected,\n",
    "        'pct_rejected': n_rejected / n_total * 100,\n",
    "        'feature_violations': feature_violation_count,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyser_faux_negatifs(\n",
    "    X: pd.DataFrame,\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    mask_accepted: np.ndarray,\n",
    "    qualite: str,\n",
    "    tolerance_modele: float,\n",
    "    X_engineered: pd.DataFrame = None,\n",
    "    selected_features: List[str] = None,\n",
    "    save_dir: str = None,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Deep dive into False Negatives (Missed Flags).\n",
    "    Uses statistical tests to see if FN samples have distinct feature distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    cible = CIBLES_D43.get(qualite, 300)\n",
    "    tolerance_terrain = TOLERANCE_TERRAIN\n",
    "    \n",
    "    # analyze only within accepted data (since rejected data is already handled)\n",
    "    y_true_acc = y_true[mask_accepted]\n",
    "    y_pred_acc = y_pred[mask_accepted]\n",
    "    X_acc = X[mask_accepted].copy()\n",
    "    \n",
    "    if X_engineered is not None:\n",
    "        X_eng_acc = X_engineered[mask_accepted].copy()\n",
    "    else:\n",
    "        X_eng_acc = None\n",
    "    \n",
    "    # logic definitions for FN/TP/TN\n",
    "    flags_reels = (y_true_acc < cible - tolerance_terrain) | (y_true_acc > cible + tolerance_terrain)\n",
    "    flags_pred = (y_pred_acc < cible - tolerance_modele) | (y_pred_acc > cible + tolerance_modele)\n",
    "    \n",
    "    mask_FN = flags_reels & ~flags_pred  # Real flag missed by model\n",
    "    mask_TP = flags_reels & flags_pred   # Real flag caught\n",
    "    mask_TN = ~flags_reels & ~flags_pred # Correctly identified as normal\n",
    "    mask_autres = ~mask_FN  # Everything else\n",
    "    \n",
    "    n_FN = np.sum(mask_FN)\n",
    "    n_TP = np.sum(mask_TP)\n",
    "    n_total_flags = np.sum(flags_reels)\n",
    "    \n",
    "    \n",
    "    print(f\"Total flags rÃ©els: {n_total_flags}\")\n",
    "    print(f\"TP (dÃ©tectÃ©s): {n_TP}\")\n",
    "    print(f\"FN (manquÃ©s): {n_FN}\")\n",
    "    \n",
    "    if n_FN == 0:\n",
    "        print(\"\\nâœ… Aucun Faux NÃ©gatif ! Tous les flags ont Ã©tÃ© dÃ©tectÃ©s.\")\n",
    "        return {'n_FN': 0, 'message': 'Aucun FN'}\n",
    "    \n",
    "    # listing specific cases for manual review\n",
    "    \n",
    "    fn_indices = np.where(mask_FN)[0]\n",
    "    \n",
    "    print(f\"\\n{'#':<4} {'Y_rÃ©el':<10} {'Y_prÃ©d':<10} {'Erreur':<10} {'Ã‰cart/cible':<12} {'Direction'}\")\n",
    "    \n",
    "    fn_details = []\n",
    "    for i, idx in enumerate(fn_indices):\n",
    "        y_r = y_true_acc[idx]\n",
    "        y_p = y_pred_acc[idx]\n",
    "        err = abs(y_r - y_p)\n",
    "        ecart_cible = y_r - cible\n",
    "        direction = \"HAUT â†‘\" if y_r > cible else \"BAS â†“\"\n",
    "        \n",
    "        fn_details.append({\n",
    "            'idx': idx,\n",
    "            'y_true': y_r,\n",
    "            'y_pred': y_p,\n",
    "            'erreur': err,\n",
    "            'ecart_cible': ecart_cible,\n",
    "            'direction': 'haut' if y_r > cible else 'bas'\n",
    "        })\n",
    "        \n",
    "        print(f\"{i+1:<4} {y_r:<10.1f} {y_p:<10.1f} {err:<10.1f} {ecart_cible:<+12.1f} {direction}\")\n",
    "    \n",
    "    # Statistical Comparison: FN vs The Rest\n",
    "    \n",
    "    features_originales = [f for f in FEATURES_POOL if f in X_acc.columns]\n",
    "    \n",
    "    print(f\"{'Feature':<25} {'MÃ©d. FN':<12} {'MÃ©d. Autres':<12} {'Diff %':<10} {'p-value':<10} {'Signif.'}\")\n",
    "    \n",
    "    significant_features = []\n",
    "    feature_stats = {}\n",
    "    \n",
    "    # Mann-Whitney U test to check if FN samples come from a different distribution\n",
    "    for feature in features_originales:\n",
    "        if X_acc[feature].dtype in ['object', 'category']:\n",
    "            continue\n",
    "        \n",
    "        fn_values = X_acc.loc[mask_FN, feature].dropna()\n",
    "        autres_values = X_acc.loc[mask_autres, feature].dropna()\n",
    "        \n",
    "        if len(fn_values) >= 2 and len(autres_values) >= 5:\n",
    "            try:\n",
    "                stat, p_value = scipy_stats.mannwhitneyu(fn_values, autres_values, alternative='two-sided')\n",
    "                \n",
    "                median_fn = fn_values.median()\n",
    "                median_autres = autres_values.median()\n",
    "                diff_pct = ((median_fn - median_autres) / (abs(median_autres) + 1e-10)) * 100\n",
    "                \n",
    "                feature_stats[feature] = {\n",
    "                    'median_fn': median_fn,\n",
    "                    'median_autres': median_autres,\n",
    "                    'mean_fn': fn_values.mean(),\n",
    "                    'mean_autres': autres_values.mean(),\n",
    "                    'std_fn': fn_values.std(),\n",
    "                    'std_autres': autres_values.std(),\n",
    "                    'diff_pct': diff_pct,\n",
    "                    'p_value': p_value,\n",
    "                }\n",
    "                \n",
    "                # permissive p-value threshold (0.1) due to small sample size of FNs\n",
    "                sig = \"âš ï¸ OUI\" if p_value < 0.1 else \"Non\"\n",
    "                if p_value < 0.1:\n",
    "                    significant_features.append((feature, p_value, diff_pct, 'original'))\n",
    "                \n",
    "                print(f\"{feature:<25} {median_fn:<12.4f} {median_autres:<12.4f} {diff_pct:<+10.1f} {p_value:<10.4f} {sig}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # similar analysis for engineered features to see if synthetic features capture the issue\n",
    "    if X_eng_acc is not None and selected_features is not None:\n",
    "        print(f\"{'Feature':<30} {'MÃ©d. FN':<12} {'MÃ©d. Autres':<12} {'Diff %':<10} {'p-value':<10} {'Signif.'}\")\n",
    "        print(\"-\" * 95)\n",
    "        \n",
    "        for feature in selected_features:\n",
    "            if feature not in X_eng_acc.columns:\n",
    "                continue\n",
    "            if X_eng_acc[feature].dtype in ['object', 'category']:\n",
    "                continue\n",
    "            \n",
    "            fn_values = X_eng_acc.loc[mask_FN, feature].dropna()\n",
    "            autres_values = X_eng_acc.loc[mask_autres, feature].dropna()\n",
    "            \n",
    "            if len(fn_values) >= 2 and len(autres_values) >= 5:\n",
    "                try:\n",
    "                    stat, p_value = scipy_stats.mannwhitneyu(fn_values, autres_values, alternative='two-sided')\n",
    "                    \n",
    "                    median_fn = fn_values.median()\n",
    "                    median_autres = autres_values.median()\n",
    "                    diff_pct = ((median_fn - median_autres) / (abs(median_autres) + 1e-10)) * 100\n",
    "                    \n",
    "                    feature_stats[f\"eng_{feature}\"] = {\n",
    "                        'median_fn': median_fn,\n",
    "                        'median_autres': median_autres,\n",
    "                        'diff_pct': diff_pct,\n",
    "                        'p_value': p_value,\n",
    "                    }\n",
    "                    \n",
    "                    sig = \"âš ï¸ OUI\" if p_value < 0.1 else \"Non\"\n",
    "                    if p_value < 0.1:\n",
    "                        significant_features.append((feature, p_value, diff_pct, 'engineered'))\n",
    "                    \n",
    "                    print(f\"{feature:<30} {median_fn:<12.4f} {median_autres:<12.4f} {diff_pct:<+10.1f} {p_value:<10.4f} {sig}\")\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # comparing TP vs FN to understand why some flags are caught and others missed\n",
    "    if n_TP > 0:\n",
    "        print(\"   â†’ Pourquoi certains flags sont dÃ©tectÃ©s et d'autres non?\")\n",
    "        print(f\"\\n{'Feature':<25} {'MÃ©d. FN':<12} {'MÃ©d. TP':<12} {'Diff %':<10}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for feature in features_originales[:10]:\n",
    "            if X_acc[feature].dtype in ['object', 'category']:\n",
    "                continue\n",
    "            \n",
    "            fn_values = X_acc.loc[mask_FN, feature].dropna()\n",
    "            tp_values = X_acc.loc[mask_TP, feature].dropna()\n",
    "            \n",
    "            if len(fn_values) >= 1 and len(tp_values) >= 1:\n",
    "                median_fn = fn_values.median()\n",
    "                median_tp = tp_values.median()\n",
    "                diff_pct = ((median_fn - median_tp) / (abs(median_tp) + 1e-10)) * 100\n",
    "                \n",
    "                print(f\"{feature:<25} {median_fn:<12.4f} {median_tp:<12.4f} {diff_pct:<+10.1f}\")\n",
    "\n",
    "    #Plotting distributions for significant features\n",
    "    features_to_plot = [f for f, p, d, t in significant_features if t == 'original'][:4]\n",
    "    if len(features_to_plot) < 4:\n",
    "        remaining = [f for f in features_originales if f not in features_to_plot][:4-len(features_to_plot)]\n",
    "        features_to_plot.extend(remaining)\n",
    "    \n",
    "    if len(features_to_plot) > 0:\n",
    "        n_features = min(len(features_to_plot), 4)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, n_features, figsize=(5*n_features, 10))\n",
    "        fig.suptitle(f'Analyse des Faux NÃ©gatifs - {qualite}\\n'\n",
    "                     f'(FN={n_FN} flags manquÃ©s sur {n_total_flags} flags rÃ©els)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        \n",
    "        if n_features == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        for i, feature in enumerate(features_to_plot[:n_features]):\n",
    "            # Row 1: Histograms\n",
    "            ax1 = axes[0, i]\n",
    "            \n",
    "            fn_vals = X_acc.loc[mask_FN, feature].dropna()\n",
    "            autres_vals = X_acc.loc[mask_autres, feature].dropna()\n",
    "            tp_vals = X_acc.loc[mask_TP, feature].dropna() if n_TP > 0 else np.array([])\n",
    "            \n",
    "            bins = 20\n",
    "            ax1.hist(autres_vals, bins=bins, alpha=0.5, color='gray', \n",
    "                     label=f'Autres (n={len(autres_vals)})', density=True)\n",
    "            if len(tp_vals) > 0:\n",
    "                ax1.hist(tp_vals, bins=bins, alpha=0.7, color='green', \n",
    "                         label=f'TP dÃ©tectÃ©s (n={len(tp_vals)})', density=True)\n",
    "            ax1.hist(fn_vals, bins=bins, alpha=0.8, color='red', \n",
    "                     label=f'FN manquÃ©s (n={len(fn_vals)})', density=True)\n",
    "            \n",
    "            ax1.axvline(autres_vals.median(), color='gray', linestyle='--', linewidth=2)\n",
    "            ax1.axvline(fn_vals.median(), color='red', linestyle='--', linewidth=2)\n",
    "            if len(tp_vals) > 0:\n",
    "                ax1.axvline(tp_vals.median(), color='green', linestyle='--', linewidth=2)\n",
    "            \n",
    "            p_val = feature_stats.get(feature, {}).get('p_value', 1.0)\n",
    "            title = f'{feature}'\n",
    "            if p_val < 0.1:\n",
    "                title += f'\\nâš ï¸ p={p_val:.3f}'\n",
    "            ax1.set_title(title, fontsize=10, fontweight='bold' if p_val < 0.1 else 'normal')\n",
    "            ax1.legend(fontsize=7)\n",
    "            ax1.set_xlabel(feature)\n",
    "            ax1.set_ylabel('DensitÃ©')\n",
    "            \n",
    "            # Row 2: Boxplots\n",
    "            ax2 = axes[1, i]\n",
    "            \n",
    "            data_boxplot = []\n",
    "            labels_boxplot = []\n",
    "            colors_boxplot = []\n",
    "            \n",
    "            if len(autres_vals) > 0:\n",
    "                data_boxplot.append(autres_vals)\n",
    "                labels_boxplot.append(f'Autres\\n(n={len(autres_vals)})')\n",
    "                colors_boxplot.append('lightgray')\n",
    "            if len(tp_vals) > 0:\n",
    "                data_boxplot.append(tp_vals)\n",
    "                labels_boxplot.append(f'TP\\n(n={len(tp_vals)})')\n",
    "                colors_boxplot.append('lightgreen')\n",
    "            if len(fn_vals) > 0:\n",
    "                data_boxplot.append(fn_vals)\n",
    "                labels_boxplot.append(f'FN\\n(n={len(fn_vals)})')\n",
    "                colors_boxplot.append('lightcoral')\n",
    "            \n",
    "            bp = ax2.boxplot(data_boxplot, labels=labels_boxplot, patch_artist=True)\n",
    "            for patch, color in zip(bp['boxes'], colors_boxplot):\n",
    "                patch.set_facecolor(color)\n",
    "            \n",
    "            # highlighting specific FN points on the boxplot\n",
    "            if len(fn_vals) > 0:\n",
    "                x_fn = [len(data_boxplot)] * len(fn_vals)\n",
    "                ax2.scatter(x_fn, fn_vals, c='red', s=100, marker='X', zorder=5, \n",
    "                           edgecolors='black', linewidths=1)\n",
    "            \n",
    "            ax2.set_ylabel(feature)\n",
    "            ax2.set_title(f'Distribution {feature}', fontsize=10)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            filepath = os.path.join(save_dir, f'analyse_FN_{qualite}.png')\n",
    "            plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "            print(f\"\\nðŸ’¾ Analyse FN sauvegardÃ©e: {filepath}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Zoomed plots for FN analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle(f'Zoom sur les Faux NÃ©gatifs - {qualite}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(y_true_acc[mask_TN], y_pred_acc[mask_TN], c='lightgray', alpha=0.5, \n",
    "                label=f'TN (n={np.sum(mask_TN)})', s=30)\n",
    "    if n_TP > 0:\n",
    "        ax1.scatter(y_true_acc[mask_TP], y_pred_acc[mask_TP], c='green', alpha=0.7, \n",
    "                    label=f'TP (n={n_TP})', s=60)\n",
    "    ax1.scatter(y_true_acc[mask_FN], y_pred_acc[mask_FN], c='red', marker='X', s=200, \n",
    "                label=f'FN (n={n_FN})', edgecolors='black', linewidths=1.5, zorder=10)\n",
    "    \n",
    "    min_val, max_val = y_true_acc.min() - 5, y_true_acc.max() + 5\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    ax1.axvline(cible - tolerance_terrain, color='red', linestyle='--', alpha=0.5, label='Bornes terrain')\n",
    "    ax1.axvline(cible + tolerance_terrain, color='red', linestyle='--', alpha=0.5)\n",
    "    ax1.axhline(cible - tolerance_modele, color='blue', linestyle=':', alpha=0.5, label='Bornes modÃ¨le')\n",
    "    ax1.axhline(cible + tolerance_modele, color='blue', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    ax1.set_xlabel('Y RÃ©el (D43)', fontsize=11)\n",
    "    ax1.set_ylabel('Y PrÃ©dit', fontsize=11)\n",
    "    ax1.set_title('Vue globale avec FN en rouge', fontsize=12)\n",
    "    ax1.legend(loc='upper left', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # comparing absolute errors by category\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    erreurs = np.abs(y_true_acc - y_pred_acc)\n",
    "    \n",
    "    categories = ['TN', 'TP', 'FN']\n",
    "    masks = [mask_TN, mask_TP, mask_FN]\n",
    "    colors = ['lightgray', 'lightgreen', 'lightcoral']\n",
    "    \n",
    "    data_err = [erreurs[m] for m in masks if np.sum(m) > 0]\n",
    "    labels_err = [f'{cat}\\n(n={np.sum(m)})' for cat, m in zip(categories, masks) if np.sum(m) > 0]\n",
    "    colors_err = [c for c, m in zip(colors, masks) if np.sum(m) > 0]\n",
    "    \n",
    "    bp = ax2.boxplot(data_err, labels=labels_err, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors_err):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax2.axhline(y=5, color='orange', linestyle='--', linewidth=2, label='Seuil erreur = 5')\n",
    "    ax2.set_ylabel('Erreur Absolue', fontsize=11)\n",
    "    ax2.set_title('Distribution des erreurs par catÃ©gorie', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        filepath = os.path.join(save_dir, f'zoom_FN_{qualite}.png')\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ðŸ’¾ Zoom FN sauvegardÃ©: {filepath}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Hypothesis Generation   \n",
    "    fn_haut = sum(1 for d in fn_details if d['direction'] == 'haut')\n",
    "    fn_bas = sum(1 for d in fn_details if d['direction'] == 'bas')\n",
    "    \n",
    "    print(f\"\\nðŸ“ Direction des flags manquÃ©s:\")\n",
    "    print(f\"FN au-dessus de la cible (D43 > {cible + tolerance_terrain}): {fn_haut}\")\n",
    "    print(f\"FN en-dessous de la cible (D43 < {cible - tolerance_terrain}): {fn_bas}\")\n",
    "    \n",
    "    erreur_moy_fn = np.mean([d['erreur'] for d in fn_details])\n",
    "    print(f\"\\nðŸ“ Erreur moyenne des FN: {erreur_moy_fn:.2f}\")\n",
    "    \n",
    "    if significant_features:\n",
    "        print(f\"\\nðŸŽ¯ Features significativement diffÃ©rentes pour les FN:\")\n",
    "        for feat, p_val, diff, feat_type in sorted(significant_features, key=lambda x: x[1])[:5]:\n",
    "            direction = \"PLUS Ã‰LEVÃ‰E\" if diff > 0 else \"PLUS BASSE\"\n",
    "            print(f\"   â€¢ {feat} ({feat_type}): {direction} de {abs(diff):.1f}% (p={p_val:.3f})\")\n",
    "    \n",
    "    return {\n",
    "        'n_FN': n_FN,\n",
    "        'n_TP': n_TP,\n",
    "        'fn_details': fn_details,\n",
    "        'feature_stats': feature_stats,\n",
    "        'significant_features': significant_features,\n",
    "        'erreur_moyenne_fn': erreur_moy_fn,\n",
    "        'fn_direction': {'haut': fn_haut, 'bas': fn_bas},\n",
    "    }\n",
    "\n",
    "\n",
    "def optimiser_tolerance_modele(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    mask_accepted: np.ndarray,\n",
    "    qualite: str,\n",
    "    tolerances_test: List[float] = None,\n",
    ") -> Tuple[float, Dict]:\n",
    "    \"\"\"\n",
    "    Finds the optimal model tolerance using Lexicographical Optimization.\n",
    "    Strategy: \n",
    "    1. Minimize FN (Safety first - don't miss alerts)\n",
    "    2. Then Minimize FP (Cost efficiency - don't raise false alarms)\n",
    "    \"\"\"\n",
    "    \n",
    "    if tolerances_test is None:\n",
    "        tolerances_test = list(range(3, 21))\n",
    "    \n",
    "    cible = CIBLES_D43.get(qualite, 300)\n",
    "    tolerance_terrain = TOLERANCE_TERRAIN\n",
    "    \n",
    "    y_true_acc = y_true[mask_accepted]\n",
    "    y_pred_acc = y_pred[mask_accepted]\n",
    "    \n",
    "    # Ground truth flags\n",
    "    flags_reels = (y_true_acc < cible - tolerance_terrain) | (y_true_acc > cible + tolerance_terrain)\n",
    "    n_flags_reels = np.sum(flags_reels)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ OPTIMISATION DE LA TOLÃ‰RANCE MODÃˆLE (Lexicographique)\")\n",
    "    print(f\"   StratÃ©gie: 1) Minimiser FN, 2) Puis minimiser FP\")\n",
    "    print(f\"   Cible: {cible}, TolÃ©rance terrain: Â±{tolerance_terrain}\")\n",
    "    print(f\"   Flags rÃ©els: {n_flags_reels} sur {len(y_true_acc)} acceptÃ©es\")\n",
    "    print(f\"\\n{'Tol.':<6} {'FN':<6} {'FP':<6} {'TP':<6} {'Recall':<10} {'Precision':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    resultats = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for tol in tolerances_test:\n",
    "        flags_pred = (y_pred_acc < cible - tol) | (y_pred_acc > cible + tol)\n",
    "        \n",
    "        TP = np.sum(flags_reels & flags_pred)\n",
    "        TN = np.sum(~flags_reels & ~flags_pred)\n",
    "        FP = np.sum(~flags_reels & flags_pred)\n",
    "        FN = np.sum(flags_reels & ~flags_pred)\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        resultats.append({\n",
    "            'tolerance': tol,\n",
    "            'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'borne_basse': cible - tol,\n",
    "            'borne_haute': cible + tol,\n",
    "        })\n",
    "        \n",
    "        print(f\"Â± {tol:<4} {FN:<6} {FP:<6} {TP:<6} {recall:<10.1%} {precision:<10.1%}\")\n",
    "    \n",
    "    df_results = pd.DataFrame(resultats)\n",
    "    \n",
    "    # Step 1: Find minimum FN achievable\n",
    "    fn_min = df_results['FN'].min()\n",
    "\n",
    "    # Step 2: Filter results that achieve this minimum FN\n",
    "    df_fn_min = df_results[df_results['FN'] == fn_min]\n",
    "    \n",
    "    # Step 3: Among those, find the one with minimum FP\n",
    "    fp_min_dans_fn_min = df_fn_min['FP'].min()\n",
    "    df_optimal = df_fn_min[df_fn_min['FP'] == fp_min_dans_fn_min]\n",
    "\n",
    "    # Step 4: Tie-breaker, cap tolerance at 15 and pick largest valid tolerance\n",
    "    df_optimal = df_optimal[df_optimal['tolerance'] <= 15]\n",
    "    best_idx = df_optimal['tolerance'].idxmax()\n",
    "    best = df_results.loc[best_idx].to_dict()\n",
    "    \n",
    "    print(f\"âœ… OPTIMISATION LEXICOGRAPHIQUE:\")\n",
    "    print(f\"   1. FN minimum trouvÃ©: {fn_min}\")\n",
    "    print(f\"   2. Parmi FN={fn_min}, FP minimum: {fp_min_dans_fn_min}\")\n",
    "    print(f\"   3. TolÃ©rance sÃ©lectionnÃ©e: Â± {best['tolerance']}\")\n",
    "    print(f\"\\n TOLÃ‰RANCE OPTIMALE: Â± {best['tolerance']}\")\n",
    "    print(f\"   Bornes: [{best['borne_basse']}, {best['borne_haute']}]\")\n",
    "    print(f\"   FN: {best['FN']} | FP: {best['FP']}\")\n",
    "    print(f\"   Recall: {best['recall']:.1%} | Precision: {best['precision']:.1%}\")\n",
    "    \n",
    "    return float(best['tolerance']), {\n",
    "        'all_results': df_results.to_dict('records'),\n",
    "        'best': best,\n",
    "        'cible': cible,\n",
    "        'tolerance_terrain': tolerance_terrain,\n",
    "        'n_flags_reels': n_flags_reels,\n",
    "        'fn_min': int(fn_min),\n",
    "        'fp_min_given_fn_min': int(fp_min_dans_fn_min),\n",
    "        'optimization_strategy': 'lexicographic: min(FN) then min(FP)',\n",
    "    }\n",
    "\n",
    "\n",
    "def analyser_detection_flags_avec_rejection(\n",
    "    y_true, y_pred, confidence, should_reject, should_reject_strict,\n",
    "    features_out_of_bounds, qualite, confidence_threshold=0.5,\n",
    "    tolerances_modele=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    save_dir=None, \n",
    "    optimiser_auto: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrapper that applies rejection logic BEFORE optimizing the detection tolerance.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "    y_pred_arr = np.array(y_pred)\n",
    "    \n",
    "    cible = CIBLES_D43.get(qualite, 300)\n",
    "    tolerance_terrain = TOLERANCE_TERRAIN\n",
    "    \n",
    "    mask_accepted = ~should_reject\n",
    "    n_total = len(y_true_arr)\n",
    "    n_accepted = np.sum(mask_accepted)\n",
    "    n_rejected = np.sum(should_reject)\n",
    "    n_rejected_strict = np.sum(should_reject_strict)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"ðŸš¨ ANALYSE DÃ‰TECTION FLAGS - REJECTION STRICTE - {qualite}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nðŸ›¡ï¸ SYSTÃˆME DE REJECTION (STRICT):\")\n",
    "    print(f\"   â€¢ Total: {n_total}\")\n",
    "    print(f\"   â€¢ AcceptÃ©es: {n_accepted} ({n_accepted/n_total*100:.1f}%)\")\n",
    "    print(f\"   â€¢ RejetÃ©es: {n_rejected} ({n_rejected/n_total*100:.1f}%)\")\n",
    "    \n",
    "    if n_accepted == 0:\n",
    "        print(\"\\nâŒ Toutes rejetÃ©es!\")\n",
    "        return {'error': 'Toutes rejetÃ©es', 'tolerance_optimale': 10}\n",
    "    \n",
    "    y_true_accepted = y_true_arr[mask_accepted]\n",
    "    y_pred_accepted = y_pred_arr[mask_accepted]\n",
    "    \n",
    "    abs_errors_accepted = np.abs(y_true_accepted - y_pred_accepted)\n",
    "    rmse_accepted = np.sqrt(np.mean((y_true_accepted - y_pred_accepted) ** 2))\n",
    "    \n",
    "    print(f\"\\nðŸ“Š MÃ‰TRIQUES ACCEPTÃ‰ES ({n_accepted}):\")\n",
    "    print(f\"   â€¢ RMSE: {rmse_accepted:.3f}\")\n",
    "    \n",
    "    flags_reels_accepted = (y_true_accepted < cible - tolerance_terrain) | (y_true_accepted > cible + tolerance_terrain)\n",
    "    n_flags_reels = np.sum(flags_reels_accepted)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Flags terrain: {n_flags_reels} ({n_flags_reels/n_accepted*100:.1f}%)\")\n",
    "    \n",
    "    if optimiser_auto:\n",
    "        # runs the lexicographical optimization\n",
    "        tolerance_optimale, optim_results = optimiser_tolerance_modele(\n",
    "            y_true=y_true_arr,\n",
    "            y_pred=y_pred_arr,\n",
    "            mask_accepted=mask_accepted,\n",
    "            qualite=qualite,\n",
    "            tolerances_test=list(range(3, 21)),\n",
    "        )\n",
    "        best = optim_results['best']\n",
    "    else:\n",
    "        # fallback manual testing (legacy mode)\n",
    "        print(f\"\\n{'Tol.':<6} {'FN':<6} {'FP':<6} {'Recall':<10} {'Precision':<10} {'F1':<10}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        results_par_tolerance = []\n",
    "        for tol in tolerances_modele:\n",
    "            flags_pred = (y_pred_accepted < cible - tol) | (y_pred_accepted > cible + tol)\n",
    "            \n",
    "            TP = np.sum(flags_reels_accepted & flags_pred)\n",
    "            FP = np.sum(~flags_reels_accepted & flags_pred)\n",
    "            FN = np.sum(flags_reels_accepted & ~flags_pred)\n",
    "            \n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            results_par_tolerance.append({\n",
    "                'tolerance': tol, 'precision': precision, 'recall': recall, \n",
    "                'f1': f1, 'FN': FN, 'FP': FP\n",
    "            })\n",
    "            \n",
    "            print(f\"Â± {tol:<4} {FN:<6} {FP:<6} {recall:<10.3f} {precision:<10.3f} {f1:<10.3f}\")\n",
    "        \n",
    "        df_results = pd.DataFrame(results_par_tolerance)\n",
    "        best = (\n",
    "            df_results\n",
    "            .sort_values(by=[\"FN\", \"FP\", \"f1\"], ascending=[True, True, False])\n",
    "            .iloc[0]\n",
    "            .to_dict()\n",
    "        )\n",
    "        \n",
    "        tolerance_optimale = best['tolerance']\n",
    "        optim_results = {'all_results': results_par_tolerance, 'best': best}\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸ“Œ TOLÃ‰RANCE OPTIMALE SÃ‰LECTIONNÃ‰E: Â± {tolerance_optimale}\")\n",
    "    print(f\"   â€¢ Bornes d'alerte: [{cible - tolerance_optimale}, {cible + tolerance_optimale}]\")\n",
    "    print(f\"   â€¢ FN (flags manquÃ©s): {best['FN']}\")\n",
    "    print(f\"   â€¢ FP (fausses alertes): {best['FP']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'qualite': qualite,\n",
    "        'n_accepted': n_accepted,\n",
    "        'tolerance_optimale': int(tolerance_optimale),\n",
    "        'best': best,\n",
    "        'optimization': optim_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def entrainer_modele_production(\n",
    "    qual: str, \n",
    "    combined_data: pd.DataFrame, \n",
    "    confidence_threshold: float = 0.25,\n",
    "    strict_mode: bool = True,\n",
    "    quantile_low: float = 0.05,\n",
    "    quantile_high: float = 0.95,\n",
    "    n_features_tolerance: int = 0\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Main orchestration function: \n",
    "    Prepares data -> Trains Model -> Runs all Diagnostics -> Saves Artifacts.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"model_production_{qual}.joblib\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š PrÃ©paration des donnÃ©es...\")\n",
    "    \n",
    "    data = combined_data[combined_data['#HIDDEN'] == qual].copy()\n",
    "    data['#HIDDEN'] = pd.to_datetime(data['#HIDDEN'], errors='coerce')\n",
    "    \n",
    "    # ensure numeric types for pool features\n",
    "    for col in FEATURES_POOL:\n",
    "        if col in data.columns:\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    data['#HIDDEN'] = pd.to_numeric(data['#HIDDEN'], errors='coerce')\n",
    "    \n",
    "    features_num_dispo = [f for f in FEATURES_POOL if f in data.columns]\n",
    "    features_cat_dispo = [f for f in CATEGORICAL_FEATURES.get(qual, []) if f in data.columns]\n",
    "    features_dispo = features_num_dispo + features_cat_dispo\n",
    "    \n",
    "    print(f\"   â€¢ Features: {len(features_num_dispo)} num + {len(features_cat_dispo)} cat\")\n",
    "    \n",
    "    cols_needed = features_dispo + ['#HIDDEN', '#HIDDEN', '#HIDDEN']\n",
    "    cols_needed = [c for c in cols_needed if c in data.columns]\n",
    "\n",
    "    data_clean = data.dropna(subset=cols_needed)\n",
    "\n",
    "    # applying date filter for specific quality codes (business requirement)\n",
    "    data_filtered = data_clean[\n",
    "        (data_clean['#HIDDEN'] != '#HIDDEN') |\n",
    "        (data_clean['#HIDDEN'] >= '#HIDDEN')\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"   â€¢ DonnÃ©es: {len(data_filtered)} lignes\")\n",
    "    \n",
    "    if len(data_filtered) < 100:\n",
    "        print(f\"âŒ Pas assez de donnÃ©es\")\n",
    "        return None\n",
    "    \n",
    "    X = data_filtered[features_dispo].copy()\n",
    "    y = data_filtered['#HIDDEN'].copy()\n",
    "    \n",
    "    # stratified split based on target quartiles to ensure test set representativity\n",
    "    y_quartiles = pd.qcut(y, q=4, labels=False, duplicates='drop')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y_quartiles\n",
    "    )\n",
    "    \n",
    "    print(f\"   â€¢ Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "    \n",
    "    # initializing and training the strict production model\n",
    "    model = ProductionD43Model(\n",
    "        rejection_threshold=confidence_threshold, \n",
    "        qualite=qual,\n",
    "        strict_mode=strict_mode,\n",
    "        quantile_low=quantile_low,\n",
    "        quantile_high=quantile_high,\n",
    "        n_features_tolerance=n_features_tolerance\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # inference on test set\n",
    "    details = model.predict(X_test, return_details=True)\n",
    "    y_pred = details['predictions']\n",
    "    confidence = details['confidence']\n",
    "    should_reject = details['should_reject']\n",
    "    should_reject_strict = details['should_reject_strict']\n",
    "    features_out = details['features_out_of_bounds']\n",
    "    \n",
    "    # evaluation logic\n",
    "    y_test_arr = y_test.values if hasattr(y_test, 'values') else np.array(y_test)\n",
    "    mask_accepted = ~should_reject\n",
    "    n_total = len(y_test_arr)\n",
    "    n_accepted = np.sum(mask_accepted)\n",
    "    n_rejected_strict = np.sum(should_reject_strict)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ðŸ“Š Ã‰VALUATION AVEC REJECTION STRICTE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nðŸ›¡ï¸ REJECTION:\")\n",
    "    print(f\"   â€¢ Total: {n_total}\")\n",
    "    print(f\"   â€¢ AcceptÃ©es: {n_accepted} ({n_accepted/n_total*100:.1f}%)\")\n",
    "    print(f\"   â€¢ RejetÃ©es (bounds): {n_rejected_strict} ({n_rejected_strict/n_total*100:.1f}%)\")\n",
    "    \n",
    "    if n_accepted > 0:\n",
    "        y_true_accepted = y_test_arr[mask_accepted]\n",
    "        y_pred_accepted = y_pred[mask_accepted]\n",
    "        abs_errors_accepted = np.abs(y_true_accepted - y_pred_accepted)\n",
    "        \n",
    "        rmse_accepted = np.sqrt(np.mean((y_true_accepted - y_pred_accepted) ** 2))\n",
    "        mae_accepted = np.mean(abs_errors_accepted)\n",
    "        pct_within_5 = np.mean(abs_errors_accepted <= 5) * 100\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ MÃ‰TRIQUES ACCEPTÃ‰ES ({n_accepted}):\")\n",
    "        print(f\"   â€¢ RMSE: {rmse_accepted:.3f}\")\n",
    "        print(f\"   â€¢ MAE: {mae_accepted:.3f}\")\n",
    "        print(f\"   â€¢ % err â‰¤ 5: {pct_within_5:.1f}%\")\n",
    "        \n",
    "        # quick check on how much value the rejection adds\n",
    "        rmse_global = np.sqrt(np.mean((y_test_arr - y_pred) ** 2))\n",
    "        print(f\"\\nðŸ“Š COMPARAISON:\")\n",
    "        print(f\"   â€¢ RMSE global: {rmse_global:.3f}\")\n",
    "        print(f\"   â€¢ RMSE acceptÃ©: {rmse_accepted:.3f}\")\n",
    "        print(f\"   â€¢ AmÃ©lioration: {rmse_global - rmse_accepted:.3f}\")\n",
    "    \n",
    "    # run full diagnostics suite\n",
    "    flag_results = analyser_detection_flags_avec_rejection(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred,\n",
    "        confidence=confidence,\n",
    "        should_reject=should_reject,\n",
    "        should_reject_strict=should_reject_strict,\n",
    "        features_out_of_bounds=features_out,\n",
    "        qualite=qual,\n",
    "        confidence_threshold=confidence_threshold,\n",
    "        save_dir=OUTPUT_DIR,\n",
    "        optimiser_auto=True,\n",
    "    )\n",
    "    \n",
    "    tolerance_optimale = flag_results.get('tolerance_optimale', 5)\n",
    "    \n",
    "    diagnostic_production(y_test_arr, details, qual, save_dir=OUTPUT_DIR, \n",
    "                          tolerance_modele=float(tolerance_optimale))\n",
    "    \n",
    "    analyser_rejets_stricts(X_test, y_test, details, qual, save_dir=OUTPUT_DIR)\n",
    "    \n",
    "    # prepare engineered features for deep dive analysis on false negatives\n",
    "    X_test_engineered = model._prepare_features(X_test)\n",
    "    fn_analysis = analyser_faux_negatifs(\n",
    "        X=X_test,\n",
    "        y_true=y_test_arr,\n",
    "        y_pred=y_pred,\n",
    "        mask_accepted=mask_accepted,\n",
    "        qualite=qual,\n",
    "        tolerance_modele=float(tolerance_optimale),\n",
    "        X_engineered=X_test_engineered,\n",
    "        selected_features=model.selected_features,\n",
    "        save_dir=OUTPUT_DIR,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'qualite': qual,\n",
    "        'features_pool': features_dispo,\n",
    "        'selected_features': model.selected_features,\n",
    "        'training_stats': model.training_stats,\n",
    "        'rejection_config': {\n",
    "            'strict_mode': strict_mode,\n",
    "            'quantile_low': quantile_low,\n",
    "            'quantile_high': quantile_high,\n",
    "            'n_features_tolerance': n_features_tolerance,\n",
    "            'confidence_threshold': confidence_threshold,\n",
    "        },\n",
    "        'metrics': {\n",
    "            'rmse_accepted': rmse_accepted if n_accepted > 0 else None,\n",
    "            'mae_accepted': mae_accepted if n_accepted > 0 else None,\n",
    "            'pct_within_5': pct_within_5 if n_accepted > 0 else None,\n",
    "            'pct_rejected': (n_total - n_accepted) / n_total * 100,\n",
    "        },\n",
    "        'flag_detection': {\n",
    "            'cible': flag_results.get('cible', CIBLES_D43.get(qual, 300)),\n",
    "            'tolerance_terrain': TOLERANCE_TERRAIN,\n",
    "            'tolerance_optimale': flag_results.get('tolerance_optimale', 10),\n",
    "            'borne_basse': flag_results.get('borne_basse'),\n",
    "            'borne_haute': flag_results.get('borne_haute'),\n",
    "            'FN': flag_results.get('best', {}).get('FN'),\n",
    "            'FP': flag_results.get('best', {}).get('FP'),\n",
    "        },\n",
    "        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_package, output_path)\n",
    "    print(f\"   âœ… SauvegardÃ©: {output_path}\")\n",
    "    \n",
    "    return {\n",
    "        'qualite': qual,\n",
    "        'rmse': rmse_accepted if n_accepted > 0 else None,\n",
    "        'pct_rejected': (n_total - n_accepted) / n_total * 100,\n",
    "        'pct_within_5': pct_within_5 if n_accepted > 0 else None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # adjusted to target ~10-20% rejection rate based on recent validation results\n",
    "    CONFIDENCE_THRESHOLD = 0.25\n",
    "    STRICT_MODE = False # disabled strict mode to allow for tolerance logic below\n",
    "    \n",
    "    # widened bounds (2nd-98th percentile) to keep more data while filtering extreme anomalies\n",
    "    QUANTILE_LOW = 0.02\n",
    "    QUANTILE_HIGH = 0.98\n",
    "    \n",
    "    # tolerance of 1 prevents rejecting a sample just because of a single noisy sensor\n",
    "    N_FEATURES_TOLERANCE = 1\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    âš™ï¸ Configuration:\n",
    "    â€¢ Mode strict: {STRICT_MODE}\n",
    "    â€¢ Quantiles: [{QUANTILE_LOW*100:.0f}%, {QUANTILE_HIGH*100:.0f}%]\n",
    "    â€¢ TolÃ©rance features hors bornes: {N_FEATURES_TOLERANCE}\n",
    "    â€¢ Seuil confidence: {CONFIDENCE_THRESHOLD}\n",
    "    \n",
    "    RÃˆGLE DE REJECTION:\n",
    "    â€¢ Si {N_FEATURES_TOLERANCE + 1}+ feature(s) hors [{QUANTILE_LOW*100:.0f}%, {QUANTILE_HIGH*100:.0f}%] â†’ PAS DE PRÃ‰DICTION\n",
    "    \"\"\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    if 'combined_data' in dir():\n",
    "        for qual in QUALITES:\n",
    "            # running the full pipeline: training, calibration, and diagnostic generation\n",
    "            result = entrainer_modele_production(\n",
    "                qual, \n",
    "                combined_data,\n",
    "                confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    "                strict_mode=STRICT_MODE,\n",
    "                quantile_low=QUANTILE_LOW,\n",
    "                quantile_high=QUANTILE_HIGH,\n",
    "                n_features_tolerance=N_FEATURES_TOLERANCE\n",
    "            )\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "        \n",
    "        if all_results:\n",
    "            # simple summary table to compare rejection rates and rmse across targets\n",
    "            df = pd.DataFrame(all_results)\n",
    "            print(df.to_string(index=False))\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n Variable non dÃ©finie.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
